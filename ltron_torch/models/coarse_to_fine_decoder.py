from math import log

import torch
from torch.nn import (
    Module, ModuleList, Sequential, Linear, Embedding, LayerNorm)

from ltron_torch.models.mlp import linear_stack

class CoarseToFineDecoder(Module):
    '''
    Decodes a signal with (b,c) dimensions into a (b,*shape) distribution.
    This distribution is generated by predicting each dimension in shape one
    at a time, conditioning on the previous dimensions.  Only k locations are
    decoded at each step.  This means that the computational complexity is:
    shape[0]*k + shape[1]*k + ... shape[n]*k instead of product(shape), but
    also means that the output distribution is not dense everywhere.  This is
    designed to predict distributions over a large tensor with a small (k)
    number of modes.
    '''
    def __init__(self,
        shape,
        layers=3,
        nonlinearity='gelu',
        channels=768,
        dropout=0.,
        default_k=1,
        #partial_normalize=True,
    ):
        super().__init__()
        
        self.shape = shape
        self.default_k = default_k
        #self.partial_normalize = partial_normalize
        
        self.heads = ModuleList([
            linear_stack(
                layers,
                in_channels=channels,
                hidden_channels=channels,
                out_channels=s,
                nonlinearity=nonlinearity,
                hidden_dropout=dropout,
            )
            for s in shape
        ])
        self.embeddings = ModuleList([
            Sequential(Embedding(s, channels), LayerNorm(channels))
            for s in shape[:-1]
        ])
        
        self.input_norm = LayerNorm(channels)
    
    def forward(self, x, sample_mode='max', k=None):
        '''
        input:
        b, channels
        
        output:
        b, *self.shape
        '''
        assert sample_mode in ('sample', 'max')
        b, c = x.shape
        
        if k is None:
            k = self.default_k
        
        # normalize x
        x = self.input_norm(x)
        
        # initialize output_x
        out_x = torch.zeros((b, *self.shape), device=x.device)
        
        # coordinates for the progressive densification
        out_coords = []
        
        dims = len(self.shape)
        for dim in range(dims):
            n = self.shape[dim]
            
            # forward pass for this head
            head = self.heads[dim]
            head_in_x = x.view(b, 1, c)
            b_coord = torch.arange(b, device=x.device).view(b, 1)
            if dim == 0:
                dim_k = 1
                conditional_x = 0
            else:
                dim_k = k
            head_in_x = head_in_x.expand(b, dim_k, c)
            head_out_x = head(head_in_x + conditional_x)
            
            # update the output with the result computed from this head
            b_coord = b_coord.expand(b, dim_k).reshape(-1)
            coords = (b_coord,) + tuple(out_coords)
            
            #lse_sampled = torch.logsumexp(head_out_x, dim=-1, keepdim=True)
            #lse_unsampled = log(n)
            
            # normalizing using the lse might actually be a bad idea here
            # this restricts the magnitude of the output logits, which might
            # be a problem when they are competing with other unrestricted
            # logits when this is added to other decoders
            # yeah, turned out this was terrible.
            head_shape = [b*dim_k, self.shape[dim]] + [1] * (dims-dim-1)
            #lse_shape = [b*dim_k] + [1] * (dims-dim)
            
            # this does absolutely nothing!
            # not to the sampled distribution, not to the gradients
            # this does nothing!
            # softmax is invariant to constant shifts, so unless we do the
            # thing before where we add this back in where we actually sample
            # this does absolutely nothing!
            #if self.partial_normalize:
            #    out_x = out_x - lse_unsampled
            
            out_x[coords] = (
                out_x[coords] +
                #lse_unsampled +
                head_out_x.view(head_shape) #-
                #lse_sampled.view(lse_shape)
            )
            
            # compute the conditioning feature for next head
            if dim != len(self.shape)-1:
            
                # sample/max
                if sample_mode == 'max':
                    top_x = head_out_x
                
                elif sample_mode == 'sample':
                    # gumbel trick
                    z = torch.log(-torch.log(torch.rand_like(head_out_x)))
                    top_x = head_out_x - z
                
                # sample k regions to densify
                top_x = top_x.view(b, -1)
                i = torch.topk(top_x, k, dim=-1).indices.view(-1)
                k_coord = torch.div(i, n, rounding_mode='floor')
                n_coord = i % n
                
                # update out_coords based on the locations sampled
                out_coords = [
                    oc.view(b,k)[b_coord, k_coord] for oc in out_coords]
                out_coords.append(n_coord)
                
                # compute conditional input for next head
                # recompute the entire sequence because the out_coords changed
                conditional_x = 0
                for j, oc in enumerate(out_coords):
                    embedding = self.embeddings[j]
                    conditional_x = conditional_x + embedding(oc.view(b,k))
        
        return out_x
